#------Spark conf----------#
spark.app.name=Yoda-Summaries-Spark

spark.executor.instances=15

#-----spark uses higher compression when num partitions is >2000----#
spark.sql.shuffle.partitions=2100
spark.default.parallelism=2100

#------spark sql configs-----#
spark.sql.crossJoin.enabled=false
spark.sql.autoBroadcastJoinThreshold=314572800
spark.sql.tungsten.enabled=true

#----increasing spark.executor.cores can cause OOM issues due to GC overhead --#
spark.executor.cores=1
#----yarn kills executor immediately once XmX is reached-------#
spark.yarn.executor.memoryOverhead=2048
#-----can use fat executors; depends on yarn limit which is 20G currently-----#
spark.executor.memory=5g

#-----user memory = (1-spark.memory.fraction)---------#
#-----not much required for yoda summaries as no metadata and all is involved-----#
#-----however reducing it to very less value can cause OOM issues------#
spark.memory.fraction=0.4
spark.memory.storageFraction=0.3

#------library confs-----#
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.classesToRegister=org.apache.hadoop.hive.serde2.columnar.BytesRefWritable,org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable,\
org.apache.hadoop.io.LongWritable,org.apache.hadoop.hive.ql.io.RCFileInputFormat,org.apache.log4j.Logger,org.apache.hadoop.conf.Configuration,\
org.joda.time.format.DateTimeFormatter,org.apache.spark.api.java.JavaSparkContext
spark.executor.extraLibraryPath=$HADOOP_HOME/lib/hadoop-lzo-0.6.0.2.2.0.0-2041.jar:$HADOOP_HOME/lib/native/Linux-amd64-64
spark.sql.warehouse.dir=/user/hive/warehouse
spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:+UseStringDeduplication

#------to handle fewer drops over network on our cluster-----#
spark.task.maxFailures=20
spark.shuffle.io.maxRetries=20
spark.rdd.compress=true
spark.sql.broadcastTimeout=3600
spark.core.connection.ack.wait.timeout=600
spark.rpc.askTimeout=2000s
spark.network.timeout=600s
spark.executor.heartbeatInterval=100s

#-----ensuring we get enough resources before starting off-----#
#-----early investment pays off in long run :P -----------------#
spark.scheduler.minRegisteredResourcesRatio=0.9
spark.scheduler.maxRegisteredResourcesWaitingTime=30s

#------scheduling inside spark-actions to run PI-PE tasks parallelly------#
spark.scheduler.mode=FAIR
#spark.scheduler.allocation.file=fair-scheduler.xml

#-------keep in mind joins may be skewed; set tighter speculation confs------#
spark.speculation=true
spark.speculation.multiplier=2.5
spark.speculation.quantile=0.95

#----summaries do not require multi threading in executors-----#
spark.task.cpus=1


#----in yarn cluster mode following driver configs need to be passed while submitting-----#
#----note that these are mentioned here just for info and will not take effect once job is launched on yarn-----#
spark.driver.memory=18g
spark.yarn.driver.memoryOverhead=2048
spark.driver.cores=1
spark.yarn.queue=dwh
spark.yarn.maxAppAttempts=1
spark.driver.extraJavaOptions=-XX:+UseG1GC -XX:+UseStringDeduplication
spark.driver.maxResultSize=4g